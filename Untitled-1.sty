






# The multivariate normal distribution

\begin{prop}
The density of the multivariate normal distribution of a random vector $\symbf{X} = (X_1,\ldots, X_p)$ is given by  

\begin{eqnarray*}
f(\symbf{x})= f(x_{1},\ldots,x_{p}) = =\frac{1}{(2\pi )^{\frac{p}{2}}|\symbf{\Sigma}|^{{{\frac{1}{2}}}}}e^{-\frac{1}{2}(\symbf{x}-\symbf{\mu} )^T\symbf{\Sigma} ^{-1}(\symbf{x}-\symbf{\mu} )}
\end{eqnarray*}

for $x\in\mathbb{R}^p$ where $E\left( \symbf{X}\right) =\symbf{\mu}$ and $cov(\symbf{X})=E\left( (\symbf{X}-\symbf{\mu})(\symbf{X}-\symbf{\mu})^T\right) =\symbf{\Sigma}$.


\end{prop}

<!-- \pause !-->

Note that  $\symbf{\Sigma}$ is a covariance matrix, and not simply a scalar. The $ij$-th element of $\symbf{\Sigma}$ is the covariance between $X_i$ and $X_j$, and so $\symbf{\Sigma}$ contains all the covariances related to $\symbf{X}$.



# Visualising a bivariate distribution

Let $\symbf{X} \sim \mathcal{MVN}(\symbf{\mu},\symbf{\Sigma})$ where $\symbf{\mu} = \left(\begin{matrix} \mu_1 \\ \mu_2 \end{matrix}\right)=\left(\begin{matrix} 0 \\ 0 \end{matrix}\right)$ and $\symbf{\Sigma}=\left(\begin{matrix} 1 & 0 \\ 0 & 1 \end{matrix}\right)$. 

:::: {.columns}

::: {.column width="48%"}
\centering
\begin{figure}
\includegraphics[width=1\linewidth]{_data_raw/img/3d_plot.pdf} 
\caption{A 3D representation}
\end{figure}
:::

::: {.column width="4%"}

:::

::: {.column width="48%"}
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{_data_raw/img/contour_plot.pdf}
\caption{Contour plot}
\end{figure}
:::
::::




# Moment-generating function

- A moment-generating function (MGF) is defined as $\mathrm{E}[e^{t'\symbf{X}}]$, where $t$ is a constant. It is said to exist if the expectation is finite in the region of $t=0$.

- The MGF is so named because its $r$-th derivative evaluated at 0 is equal to the $r$-th moment, $\mathrm{E}[X^r]$.

- We won't use it to calculate moments, however, but rather to find the distributions of random variables as, if it exists, the moment-generating function uniquely determines the distribution.

# Linear transformation of MVN random variable: Theorem

\begin{theo}
    If $\symbf{X} \sim N(\symbf{\mu}, \symbf{\Sigma})$, then for any comformable matrix of constants $\symbf{C}$ we have that
$\symbf{Y}=\symbf{C} \symbf{X} \sim N\left(\symbf{C} \symbf{\mu}, \symbf{C} \symbf{\Sigma} \symbf{C}^{T}\right)$.
\end{theo}



<!-- \pause !-->

- This is of relevance later when we take linear combinations of the response variable, which is normally distributed (for example, to obtain estimates for $\beta$).




# Linear transformation of a MVN random variable: Proof

\begin{proof}

The mgf of $\symbf{X}$ is: 

\begin{equation*}
M_{\symbf{X}}(\symbf{t})=e\symbf{^{t^T\mu +\frac{1}{2}t^{\prime
}\Sigma t}}
\end{equation*}

The mgf of $\boldsymbol{Y}$ is: 
\begin{eqnarray*}
M_{\boldsymbol{Y}}(\boldsymbol t) &=&E\left( e^{\boldsymbol  t^T\boldsymbol{Y}}\right) =E\left( e^{%
\boldsymbol{t^T\color{red}{CX}}}\right)  \\
&=&E\left( e^{(\boldsymbol{\color{red}{C^Tt})^TX}}\right) =e^{(\boldsymbol{%
\color{red}{C^Tt})^T\mu +\frac{1}{2}(\color{red}{C^Tt})^T\Sigma
(\color{red}{C^Tt})}} \\
&=&e^{\boldsymbol{t^T\color{red}{C\mu} +\frac{1}{2}t^T\color{red}{C\Sigma C^T}t}}
\end{eqnarray*}%


This is the mgf of a multivariate normal with mean $\symbf{C\mu }$ and
covariance matrix $\symbf{C\Sigma C^T}$. Thus $\symbf{Y=CX\sim }N%
\symbf{(\color{red}{C\mu} ,\color{red}{C\Sigma C^T})}$.


\end{proof}


# Trivial example: Identity matrix

Suppose $\symbf{Y}=\symbf{CX}$ where $\symbf{X} \sim \mathcal{M V N}(\symbf{\mu}, \symbf{\Sigma})$


\begin{ex}
    If $\symbf{C}=\symbf{I}_p$, then \\
    $$E(\symbf{Y})=\symbf{I_p}\symbf{\mu} =\symbf{\mu}$$
    $$\operatorname{cov}(\symbf{Y})=\symbf{I}_p \symbf{\Sigma} \symbf{I}_p^T=\symbf{\Sigma}$$
\end{ex}

# Simple example: Individual element

\centering
\includegraphics[width=.7\linewidth]{_data_raw/img/Examples/Theorem2_2.pdf}

# Example: First two elements

\includegraphics[width=.7\linewidth]{_data_raw/img/Examples/Theorem2_3.pdf}

# Example: All elements except the first

\includegraphics[width=.7\linewidth]{_data_raw/img/Examples/Theorem2_4.pdf}

# Example: Sum of all elements

\includegraphics[width=.7\linewidth]{_data_raw/img/Examples/Theorem2_5.pdf}


# MVN partitions are MVN: Theorem

\begin{theo}
    Let $\symbf{X} \sim N(\symbf{\mu}, \symbf{\Sigma})$
and let $\symbf{X}$ be partitioned into $\symbf{X}=\left(\begin{array}{c}\symbf{X}^{(1)} \\ \symbf{X}^{(2)}\end{array}\right)$
then $\quad \symbf{X}_{(q \times 1)}^{(1)} \sim N\left(\symbf{\mu}^{(1)}, \symbf{\Sigma}_{11}\right)$ and $\symbf{X}_{(r \times 1)}^{(2)} \sim N\left(\symbf{\mu}^{(2)}, \symbf{\Sigma}_{22}\right)$ where

$$
\begin{array}{l}
\symbf{\mu}=\left(\begin{array}{c}
\symbf{\mu}^{(1)} \\
\symbf{\mu}^{(2)}
\end{array}\right)_{r}^{q} \\
\symbf{\Sigma}=\left(\begin{array}{cc}
\symbf{\Sigma}_{11} & \symbf{\Sigma}_{21} \\
\symbf{\Sigma}_{21} & \symbf{\Sigma}_{22}
\end{array}\right)_{r}^{q}
\end{array}
$$

\end{theo}

<!-- \pause !-->

- This arises primarily when we wish to obtain the distribution of one or more elements of a random vector that we already know is normally distributed, such as for the regression coefficients (i.e. if $\symbf{\beta}\sim \mathrm{MVN}$, then $\beta_i$ is normally distributed).

# MVN partitions are MVN: Proof

Proof :
$$
\begin{array}{l}
Y=C X \\
C=\left(I_{q}, 0\right)
\end{array}
$$

then

$$
\begin{aligned}
\symbf{Y}=& \symbf{C X}=\left(\symbf{I}_{q}, \symbf{0}\right)\left(\begin{array}{c}
\symbf{X}^{(1)} \\
\symbf{X}^{(2)}
\end{array}\right)=\symbf{X}^{(1)} \\
\symbf{C} \symbf{\mu} &=\left(\symbf{I}_{q}, \symbf{0}\right)\left(\begin{array}{c}
\symbf{\mu}^{(1)} \\
\symbf{\mu}^{(2)}
\end{array}\right)=\symbf{\mu}^{(1)} \\
\symbf{C \Sigma C}^{T} &=\left(\symbf{I}_{q}, \symbf{0}\right)\left(\begin{array}{cc}
\symbf{\Sigma}_{11} & \symbf{\Sigma}_{21} \\
\symbf{\Sigma}_{21} & \symbf{\Sigma}_{22}
\end{array}\right)\left(\begin{array}{c}
\symbf{I}_{q}^{T} \\
\symbf{0}^{T}
\end{array}\right)=\symbf{\Sigma}_{11}
\end{aligned}
$$

Thus $\symbf{Y}=\symbf{X}^{(1)} \sim N\left(\symbf{\mu}^{(1)}, \symbf{\Sigma}_{11}\right) .$ Similarly $\symbf{Y}=\symbf{X}^{(2)} \sim N\left(\symbf{\mu}^{(2)}, \symbf{\Sigma}_{22}\right)$

# Example: bivariate MVN

$$
f\left(x_{1}, x_{2}\right)=\frac{1}{(2 \pi)|\boldsymbol{\Sigma}| \frac{1}{2}} e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}
$$
with
$$
\begin{aligned}
X_{1} & \sim N\left(\mu_{1}, \sigma_{1}^{2}\right) \\
X_{2} & \sim N\left(\mu_{2}, \sigma_{2}^{2}\right)
\end{aligned}
$$

# Idempotent quadratic product of standard MVN is $\chi^2$: theorem

\begin{theo}
    If $\symbf{Y}_{(n\times 1)}\sim N(\symbf{O},\symbf{I}_{n})$, then $\symbf{Y^{\prime }AY}\sim\chi _{k}^{2}$, where $\symbf{A}$ is idempotent of rank $k$. 
\end{theo}

# Idempotent quadratic product of standard MVN is $\chi^2$: proof
  
Since $\symbf{A}$ is idempotent, there exists an orthonormal matrix $\symbf{P}$ such that $\symbf{P}^{\prime}\symbf{A}\symbf{P}=\left( \begin{array}{cc}I_{k} & O \\ O & O \end{array} \right)$. 

Let $\symbf{Z}=\symbf{P}^{\prime}\symbf{Y}$. Then $\symbf{Z}\sim N(\symbf{P}^{\prime}\symbf{O},\symbf{P}^{\prime}\symbf{I}\symbf{P})$. But $\symbf{P}^{\prime}\symbf{I}\symbf{P}=\symbf{P}^{\prime }\symbf{P}=\symbf{I}$ because $\symbf{P}$ is
orthonormal, \textit{i.e.} $\symbf{Z}\sim N(\symbf{O},\symbf{I})$. 

Thus 
\begin{eqnarray*}
\symbf{Y}^{\prime}\symbf{A}\symbf{Y} &=&\symbf{Z}^{\prime}\symbf{P}^{\prime}\symbf{A}\symbf{P}\symbf{Z} \\
&=&\symbf{Z}^{\prime}\left( 
\begin{array}{cc}
\symbf{I}_{k} & \symbf{O} \\ 
\symbf{O} & \symbf{O}
\end{array}
\right) \symbf{Z} \\
&=&\left( \symbf{Z}_{1}^{\prime }\symbf{Z}_{2}^{\prime }\right) \symbf{
\left( 
\begin{array}{cc}
\symbf{I}_{k} & \symbf{O} \\ 
\symbf{O} & \symbf{O}
\end{array}
\right) }\left( \symbf{Z}_{1}\symbf{Z}_{2}\right)  \\
&=&\symbf{Z}_{1}^{\prime}\symbf{I}_{k}\symbf{Z}_{1} \\
&=&\sum\nolimits_{i=1}^{k}Z_{i}^{2}
\end{eqnarray*}

but $\symbf{Z} \sim N(\symbf{O}, \symbf{I})\Longrightarrow$ all $Z_{i}$ are independent.
Thus $\symbf{Y}^{\prime}\symbf{A}\symbf{Y}=\symbf{Z}_{1}^{\prime}\symbf{Z}_{1}$
=$\sum\nolimits_{i=1}^{k}\symbf{Z}_{1}^{2}\sim \chi _{k}^{2}$. 